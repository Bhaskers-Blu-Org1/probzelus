open Infer_ds_nogc

let node delay_kalman (prob, (m, b, yobs)) =
  let rec automaton
      | Init ->
          do xt = assume_constant "x1" (mgaussian 0. 100.)
          then Run
      | Run ->
          do xt = assume_conditional ("x" ^ string_of_int t) (last xt) (affine_mean_gaussian m b 1.)
          done
      end
  and () = observe_conditional prob ("y" ^ string_of_int t) xt (gaussian_mean_gaussian 1.) yobs
  and t = 1 fby (t + 1)
  (* and init xt = assume_constant "x0" (mgaussian 0. 1.) *)
  in
  xt

let node main (tr, observed) =
  let rec t = 1. fby (t +. 1.) in

  let x_d = infer 1 delay_kalman (1., 0., observed) in

  let error = (Aux.get_mean (Distribution.draw x_d) -. tr) ** 2. in
  let rec total_error = error -> (pre total_error) +. error in
  let mse = total_error /. t in

  let memory = Aux.get_memory () in
  let rec time = (Aux.get_time ()) -> (Aux.get_time ()) -. (pre time) in

  ((string_of_float mse) ^ ", " ^ (string_of_float memory) ^ ", " ^ (string_of_float time) ^ "\n")

